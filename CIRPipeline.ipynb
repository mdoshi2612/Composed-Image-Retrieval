{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262620f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "import torch.nn as nn\n",
    "## disable warnings\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "## Imaging  library\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "## For video display\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d672827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/biplab/phduser2/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## Initiating tokenizer and encoder.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip-vit-large-patch14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m text_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPTextModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip-vit-large-patch14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m## Initiating the VAE\u001b[39;00m\n\u001b[1;32m     10\u001b[0m vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompVis/stable-diffusion-v1-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2053\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "## Import the CLIP artifacts\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"clip-vit-large-patch14\").to(\"cuda\")\n",
    "\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(\"cuda\")\n",
    "\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(\"cuda\")\n",
    "\n",
    "# CLIPImage Processor\n",
    "processor = CLIPProcessor.from_pretrained(\"clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"clip-vit-large-patch14\")\n",
    "\n",
    "# Initialize device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIRPipeline(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, text_encoder, vae, scheduler, unet, processor, image_encoder, device):\n",
    "        super(CIRPipeline, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder.to(device)\n",
    "        self.vae = vae.to(device)\n",
    "        self.scheduler = scheduler\n",
    "        self.unet = unet.to(device)\n",
    "        self.processor = processor\n",
    "        self.image_encoder = image_encoder.to(device)\n",
    "        \n",
    "        # Freeze all other models\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.unet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.text_layer = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 768)\n",
    "        ).to(device)\n",
    "\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 768)\n",
    "        ).to(device)\n",
    "\n",
    "        self.combined_layer = nn.Sequential(\n",
    "            nn.Linear(78, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 77)\n",
    "        ).to(device)        \n",
    "            \n",
    "    def load_image(self, p):\n",
    "        '''\n",
    "        Function to load images from a defined path\n",
    "        '''\n",
    "        return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "    def pil_to_latents(self, image):\n",
    "        '''\n",
    "        Function to convert image to latents\n",
    "        '''\n",
    "        init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "        init_image = init_image.to(device=\"cuda\")\n",
    "        init_latent_dist = self.vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "        return init_latent_dist\n",
    "\n",
    "    def latents_to_pil(self, latents):\n",
    "        '''\n",
    "        Function to convert latents to images\n",
    "        '''\n",
    "        latents = (1 / 0.18215) * latents\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        return pil_images\n",
    "\n",
    "    def image_enc(self, images):\n",
    "        input = self.processor(images = images, return_tensors = \"pt\").to(device)\n",
    "        return self.image_encoder.get_image_features(**input)\n",
    "\n",
    "    def text_enc(self, prompts, maxlen=None):\n",
    "        '''\n",
    "        A function to take a texual prompt and convert it into embeddings\n",
    "        '''\n",
    "        if maxlen is None: maxlen = self.tokenizer.model_max_length\n",
    "        inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
    "        return self.text_encoder(inp.input_ids.to(\"cuda\"))[0]\n",
    "    \n",
    "    def forward(self, images, prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n",
    "        \"\"\"\n",
    "        Diffusion process to convert prompt to image\n",
    "        \"\"\"\n",
    "\n",
    "        # Defining batch size\n",
    "        bs = len(prompts)\n",
    "\n",
    "        # Converting textual prompts to embedding\n",
    "        text = self.combine_inputs(images, prompts)\n",
    "        print(\"Combined Embeddings shape\", text.size())\n",
    "        # Adding an unconditional prompt , helps in the generation process\n",
    "        uncond =  self.text_enc([\"\"] * bs, text.shape[1])\n",
    "        emb = torch.cat([uncond, text])\n",
    "        print(emb.shape)\n",
    "\n",
    "        # Setting the seed\n",
    "        if seed: torch.manual_seed(seed)\n",
    "\n",
    "        # Initiating random noise\n",
    "        latents = torch.randn((bs, self.unet.config.in_channels, dim//8, dim//8))\n",
    "        print(latents.size())\n",
    "\n",
    "        # Setting number of steps in scheduler\n",
    "        self.scheduler.set_timesteps(steps)\n",
    "\n",
    "        # Adding noise to the latents\n",
    "        latents = latents.to(\"cuda\") * scheduler.init_noise_sigma\n",
    "\n",
    "        # Iterating through defined steps\n",
    "        for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            # We need to scale the i/p latents to match the variance\n",
    "            inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "            # print(inp.size(), emb.size())\n",
    "\n",
    "            # Predicting noise residual using U-Net\n",
    "            with torch.no_grad(): u,t = self.unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "\n",
    "            # Performing Guidance\n",
    "            pred = u + g*(t-u)\n",
    "\n",
    "            # Conditioning  the latents\n",
    "            latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "        # Returning the latent representation to output an image of 3x512x512\n",
    "        return self.latents_to_pil(latents)\n",
    "\n",
    "    def combine_inputs(self, images, prompts):\n",
    "        # Get batch size\n",
    "        bs = len(prompts)\n",
    "\n",
    "        # Get the text encoding\n",
    "        text_encoding = self.text_enc(prompts)\n",
    "\n",
    "        # Get image encoding\n",
    "        image_encoding = self.image_enc(images)\n",
    "            \n",
    "        text_encoding = self.text_layer(text_encoding.view(-1, 768)).reshape(bs, -1, 768)\n",
    "        image_encoding = self.image_layer(image_encoding.view(-1, 768)).reshape(bs, -1, 768)\n",
    "\n",
    "        combined_embedding = torch.cat([text_encoding, image_encoding], axis = 1)\n",
    "\n",
    "        combined_embedding = self.combined_layer(combined_embedding.view(-1, 78)).reshape(bs, 77, 768)\n",
    "        return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d377ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CIRPipeline(tokenizer, text_encoder, vae, scheduler, unet, processor, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipeline.load_image('images/lena.png')\n",
    "latents = pipeline.pil_to_latents(image)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "# Plot the images in the 1x4x64x64 matrix in four different subplots\n",
    "for i in range(4):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    axs[row, col].imshow(latents.cpu().detach()[0, i], cmap='gray')\n",
    "    axs[row, col].set_title(f'Image {i + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb35809-515f-46bd-aa94-59450d48bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embedding = pipeline.combine_inputs(image, [\"Lena it is\"])\n",
    "\n",
    "print(combined_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil = pipeline.latents_to_pil(latents)[0]\n",
    "plt.imshow(pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12a4a0-3643-4b84-ab47-4dc62b239945",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipeline(image, [\"Dressed up dogs\"])\n",
    "for img in images:display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc0dad-5903-482f-85bb-d14610489369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
