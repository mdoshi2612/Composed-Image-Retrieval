{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb959c0-c170-4ee8-9173-c69805025c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import RuntimeWarning\n",
    "from builtins import NotImplementedError\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import json\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "from pprint import pprint as print\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 1000000000\n",
    "PIL.Image.warnings.simplefilter('error', PIL.Image.DecompressionBombWarning)\n",
    "\n",
    "# For Aux Annotation in CIRR\n",
    "WORD_REPLACE = {\n",
    "  '[c] None existed': 'noneexisted',\n",
    "  '[cr0] Nothing worth mentioning': 'nothingworth',\n",
    "  '[cr1] Covered in query': 'coveredinquery',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526f2f94-9315-437c-9ce5-6d39555e8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2fff11-a022-4894-a48c-ec5998b1086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Base class for a dataset.\n",
    "  This portion is based on the TIRG implementation,\n",
    "  see https://github.com/google/tirg.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super(BaseDataset, self).__init__()\n",
    "    self.imgs = []\n",
    "    self.test_queries = []\n",
    "\n",
    "    # self.logger = logger\n",
    "    # self.logger.write(''); self.logger.write('Start init BaseDataset class...')\n",
    "  \n",
    "  def get_loader(self, batch_size, shuffle=False, drop_last=False):\n",
    "    # self.logger.write('\\nNum_worker: %i, pin_memory: %s' % (num_workers, str(pin_memory)))\n",
    "    return torch.utils.data.DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=lambda i: i)\n",
    "\n",
    "  def get_all_texts(self):\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def get_test_queries(self):\n",
    "    return self.test_queries if self.split != 'train' else self.train_queries\n",
    "\n",
    "  def generate_random_query_target(self):\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def get_img(self, idx, raw_img=False):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.generate_random_query_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57874a72-7890-4129-94c3-d01e349c2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIRR(BaseDataset):\n",
    "  \"\"\" The CIRR dataset.\n",
    "  This is partially based on TIRG implementation and Fashion-IQ implementation\n",
    "  see https://github.com/google/tirg, https://github.com/XiaoxiaoGuo/fashion-iq.\n",
    "  \"\"\"\n",
    "  def __init__(self, path, split='train', val_loader=None, transform=None, tokenizer=None):\n",
    "    super(CIRR, self).__init__()\n",
    "    '''stable dataset version, DO NOT CHANGE unless you are sure\n",
    "    corresponding dataset version can be found in our repository\n",
    "    https://github.com/Cuberick-Orion/CIRR/tree/cirr_dataset\n",
    "    '''\n",
    "    self.version = 'rc2' \n",
    "      \n",
    "    assert split in ['train', 'val', 'test1']\n",
    "    self.split = split\n",
    "\n",
    "    assert val_loader in [None, 'img+txt', 'img'] # if None, then proceed as original, if otherwise, return will be different\n",
    "    self.val_loader = val_loader\n",
    "\n",
    "    self.transform = transform\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.img_path = path\n",
    "    data = { # hold all data read from json files\n",
    "        'image_splits': {},\n",
    "        'captions': {},\n",
    "        'captions_ext': {} \n",
    "    }\n",
    "     # load the corresponding json files\n",
    "    for subfolder_name in data:\n",
    "      print(subfolder_name)\n",
    "      print(os.listdir(path + '/' + subfolder_name))\n",
    "      for json_name in os.listdir(path + '/' + subfolder_name):\n",
    "        if (split == 'train' and 'train' in json_name) \\\n",
    "        or (split == 'val' and 'val' in json_name) \\\n",
    "        or (split == 'test1' and 'test' in json_name):\n",
    "          json_load = json.load(open(path + '/' + subfolder_name + '/' + json_name))\n",
    "          data[subfolder_name][json_name] = json_load\n",
    "    \n",
    "    imgs = []\n",
    "    asin2id = {}; id2asin=[]\n",
    "    for json_name in data['image_splits']:\n",
    "      for asin_,img_path_ in data['image_splits'][json_name].items():\n",
    "        asin2id[asin_] = len(imgs)\n",
    "        id2asin.append(asin_)\n",
    "        imgs += [{\n",
    "          'asin': asin_,\n",
    "          'img_feat_res152_path': os.path.join(self.img_path, 'img_feat_res152', img_path_.replace('.png','.pkl')),\n",
    "          'captions': [asin2id[asin_]],\n",
    "          # 'img_raw_path': os.path.join(self.img_path, 'img_raw_filtered', img_path_), #! Uncomment this line if raw img is downloaded\n",
    "        }]\n",
    "\n",
    "    # process queries from loaded data\n",
    "    queries = []\n",
    "    for json_name in data['captions']:\n",
    "      for query in data['captions'][json_name]:\n",
    "        if self.split != 'test1':\n",
    "          query['source_id'] = asin2id[query['reference']]\n",
    "          query['target_id'] = asin2id[query['target_hard']]\n",
    "          query['captions'] = [query['caption']]\n",
    "          query['target_soft_id'] = {asin2id[kkk]:vvv for kkk,vvv in query['target_soft'].items()}\n",
    "          queries += [query]\n",
    "        else:\n",
    "          query['source_id'] = asin2id[query['reference']]\n",
    "          query['captions'] = [query['caption']]\n",
    "          queries += [query]\n",
    "\n",
    "    # add Aux Annoation from cap.ext\n",
    "    queries_temp = {qqq['pairid']:qqq for qqq in queries} \n",
    "    for kkk,qqq in queries_temp.items():\n",
    "      queries_temp[kkk]['caption_extend'] = None\n",
    "    for json_name in data['captions_ext']:\n",
    "      for query in data['captions_ext'][json_name]:\n",
    "        query_cap_ext_ = {}\n",
    "        for kkk_,vvv_ in query['caption_extend'].items():\n",
    "          if vvv_ in WORD_REPLACE.keys():\n",
    "            query_cap_ext_[kkk_] = WORD_REPLACE[vvv_]\n",
    "          else:\n",
    "            query_cap_ext_[kkk_] = vvv_\n",
    "        queries_temp[query['pairid']]['caption_extend'] = query_cap_ext_\n",
    "    queries = [qqq for kkk,qqq in queries_temp.items()]\n",
    "   \n",
    "    self.data = data\n",
    "    self.imgs = imgs\n",
    "    self.asin2id = asin2id; self.id2asin = id2asin\n",
    "    self.queries = queries\n",
    "\n",
    "    # prepare a copy of test_queries from queries\n",
    "    if split in ['train', 'val']:\n",
    "      self.test_queries = [{\n",
    "        'source_img_id': query['source_id'],\n",
    "        'target_img_id': query['target_id'],\n",
    "        'source_caption': query['source_id'],\n",
    "        'target_caption': query['target_id'],\n",
    "        'target_caption_soft': query['target_soft_id'],\n",
    "        'set_member_idx': [self.asin2id[ii] for ii in query['img_set']['members'] if ii != query['reference']],\n",
    "        'mod': {'str': query['captions'][0], **query['caption_extend']},\n",
    "        'caption_ext': query['caption_extend'],\n",
    "        'pairid': query['pairid']\n",
    "      } for _, query in enumerate(queries)]\n",
    "    elif split == 'test1': \n",
    "      self.test_queries = [{\n",
    "        'source_img_id': query['source_id'],\n",
    "        'source_caption': query['source_id'],\n",
    "        'set_member_idx': [self.asin2id[ii] for ii in query['img_set']['members'] if ii != query['reference']],\n",
    "        'mod': {'str': query['captions'][0], **query['caption_extend']},\n",
    "        'caption_ext': query['caption_extend'],\n",
    "        'pairid': query['pairid']\n",
    "      } for _, query in enumerate(queries)]\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split == 'train' and not self.val_loader: # in training\n",
    "      return len(self.imgs)\n",
    "    else: # in validation/test\n",
    "      if self.val_loader == 'img+txt':\n",
    "        return len(self.test_queries)\n",
    "      elif self.val_loader == 'img':\n",
    "        return len(self.imgs)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.split == 'train' and self.val_loader is None:\n",
    "      generated_ = self.generate_random_query_target()\n",
    "      if self.tokenizer:\n",
    "        oscar_input = self.oscar_tensorize(text_a=generated_['mod']['str'],\n",
    "                  text_b=None,\n",
    "                  img_feat=generated_['source_img_data'][0],\n",
    "                  cls_token_at_end=bool(self.args.model_type in ['xlnet']), # is 'bert' # xlnet has a cls token at the end\n",
    "                  cls_token=self.tokenizer.cls_token, #- '[CLS]'\n",
    "                  sep_token=self.tokenizer.sep_token,\n",
    "                  cls_token_segment_id=2 if self.args.model_type in ['xlnet'] else 0,\n",
    "                  pad_on_left=bool(self.args.model_type in ['xlnet']), # pad on the left for xlnet\n",
    "                  pad_token_segment_id=4 if self.args.model_type in ['xlnet'] else 0)  \n",
    "        return (generated_, oscar_input) # add the oscar input at the end\n",
    "      else:\n",
    "        return generated_\n",
    "    else:\n",
    "      if self.val_loader == 'img+txt':\n",
    "        generated_ = self.val_get_img_txt(idx,)\n",
    "        if self.tokenizer:\n",
    "          oscar_input = self.oscar_tensorize(text_a=generated_[1],\n",
    "                  text_b=None,\n",
    "                  img_feat=generated_[0],\n",
    "                  cls_token_at_end=bool(self.args.model_type in ['xlnet']), # is 'bert' # xlnet has a cls token at the end\n",
    "                  cls_token=self.tokenizer.cls_token, # '[CLS]'\n",
    "                  sep_token=self.tokenizer.sep_token,\n",
    "                  cls_token_segment_id=2 if self.args.model_type in ['xlnet'] else 0,\n",
    "                  pad_on_left=bool(self.args.model_type in ['xlnet']), # pad on the left for xlnet\n",
    "                  pad_token_segment_id=4 if self.args.model_type in ['xlnet'] else 0)\n",
    "          return (generated_, oscar_input) # add the oscar input at the end\n",
    "        else:\n",
    "          return generated_\n",
    "      elif self.val_loader == 'img':\n",
    "        return self.val_get_img(idx,)\n",
    "\n",
    "  def get_imgs_in_set(self, set_member_idx):\n",
    "    if not set_member_idx is None:\n",
    "      img_feats = []\n",
    "      for feat_ in self.usefeat:\n",
    "        img_feat_ = np.stack([self.get_img(d, usefeat=feat_) for d in set_member_idx])\n",
    "        img_feat_ = torch.from_numpy(img_feat_).float()\n",
    "        img_feats.append(img_feat_)\n",
    "      return img_feats\n",
    "    else:\n",
    "      return None\n",
    "  \n",
    "  def generate_random_query_target(self):\n",
    "    query_idx = random.choice(range(len(self.queries)))\n",
    "    query = self.queries[query_idx]\n",
    "    \n",
    "    mod_str = query['captions'][0]\n",
    "    mod_str_ext = query['caption_extend'] # The Aux Annotation\n",
    "    \n",
    "    other_set_member_asin = [ii for ii in query['img_set']['members'] if ii != query['reference']]\n",
    "    other_set_member_idx = [self.asin2id[ii] for ii in other_set_member_asin]\n",
    "\n",
    "    target_soft_within_set = {kkk:vvv for kkk,vvv in query['target_soft'].items() if kkk in query['img_set']['members'] and kkk != query['reference']} # only consider things in set, filter out target == reference image which is just mistakes\n",
    "    \n",
    "    random_stageI_idx = None\n",
    "\n",
    "    return {\n",
    "      'source_img_id': query['source_id'],\n",
    "      'target_img_id': query['target_id'],\n",
    "      'source_img_data': [self.get_img(query['source_id'], usefeat=i) for i in self.usefeat],\n",
    "      'target_img_data': [self.get_img(query['target_id'], usefeat=i) for i in self.usefeat],\n",
    "      'mod': {'str': mod_str, **mod_str_ext},\n",
    "      'target_caption': query['target_id'],\n",
    "      \n",
    "      'pair_id': query['pairid'],\n",
    "      'target_idx_soft': query['target_soft_id'],\n",
    "      'set_id': query['img_set']['id'],\n",
    "      'set_member_idx': other_set_member_idx, # excluding reference itself\n",
    "      'set_member_data': self.get_imgs_in_set(other_set_member_idx), # excluding reference itself\n",
    "      'set_target_soft': {other_set_member_asin.index(kkk):vvv for kkk,vvv in target_soft_within_set.items()},\n",
    "    }\n",
    "\n",
    "  def val_get_img_txt(self, idx,):\n",
    "    \"\"\"PyTorch Lightning adaptaion\n",
    "    return img+text by sequence\n",
    "    indexed by testset.test_queries\n",
    "    \"\"\"\n",
    "    assert len(self.usefeat) == 1, ValueError('Do not support multiple features as input.')\n",
    "\n",
    "    return (\n",
    "      self.get_img(self.test_queries[idx]['source_img_id'], usefeat=self.usefeat[0]),\n",
    "      self.test_queries[idx]['mod']['str'], \n",
    "      self.test_queries[idx]['mod']['0'],\n",
    "      self.test_queries[idx]['mod']['1'],\n",
    "      self.test_queries[idx]['mod']['2'],\n",
    "      self.test_queries[idx]['mod']['3'],\n",
    "    )\n",
    "\n",
    "  def val_get_img(self, idx,):\n",
    "    \"\"\"PyTorch Lightning adaptaion\n",
    "    return img by sequence\n",
    "    indexed by testset.imgs\n",
    "    \"\"\"\n",
    "    return (\n",
    "      self.get_img(idx, usefeat=self.usefeat[0]),\n",
    "    )\n",
    "  \n",
    "  def get_img(self, idx, raw_img=False, usefeat='resnet'):\n",
    "    if usefeat == 'resnet':\n",
    "      img_path = self.imgs[idx]['img_raw_path']\n",
    "      with open(img_path, 'rb') as f:\n",
    "        img = PIL.Image.open(f)\n",
    "        img = img.convert('RGB')\n",
    "      if raw_img:\n",
    "        return img\n",
    "      if self.transform:\n",
    "        img = self.transform(img)\n",
    "      return img\n",
    "    elif usefeat == 'nlvr-resnet152_w_empty':\n",
    "      \"\"\"Append empty 6-digit positional encoding\n",
    "      For OSCAR, unsqueeze the 0-dim\n",
    "      \"\"\"\n",
    "      img_path = self.imgs[idx]['img_feat_res152_path']\n",
    "      try:\n",
    "        _img_feat = pickle.load(open(img_path,'rb')) # torch.Size([2048])\n",
    "      except:\n",
    "        _img_feat = np.zeros(2048)\n",
    "      _feat = torch.from_numpy(_img_feat).float()\n",
    "      return torch.unsqueeze(torch.cat((_feat, torch.zeros(6)), dim=0), 0) # pad 0 for bbox coordinates\n",
    "    else:\n",
    "      ValueError(\"Unsupported img_feat:: %s\" % str(usefeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f0b1a6-4f77-436b-9a12-e357154bda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'image_splits'\n",
      "'captions'\n",
      "'captions_ext'\n"
     ]
    }
   ],
   "source": [
    "dataset = CIRR('./data/cirr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35be5c6-9d9e-4713-b052-46aeeabe9757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
